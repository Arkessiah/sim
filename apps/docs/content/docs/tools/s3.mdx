---
title: S3
description: View S3 files
---

import { BlockInfoCard } from "@/components/ui/block-info-card"
import { Accordion, Accordions } from "fumadocs-ui/components/accordion"


<BlockInfoCard 
  type="s3"
  color="#E0E0E0"
  icon={true}
  iconSvg={`<svg className="block-icon"
    
    
    preserveAspectRatio='xMidYMid'
    viewBox='0 0 256 310'
    
    xmlns='http://www.w3.org/2000/svg'
  >
    <path d='m20.624 53.686-20.624 10.314v181.02l20.624 10.254.124-.149v-201.297z' fill='#8c3123' />
    <path d='m131 229-110.376 26.274v-201.588l110.376 25.701z' fill='#e05243' />
    <path d='m81.178 187.866 46.818 5.96.294-.678.263-76.77-.557-.6-46.818 5.874z' fill='#8c3123' />
    <path
      d='m127.996 229.295 107.371 26.035.169-.269-.003-201.195-.17-.18-107.367 25.996z'
      fill='#8c3123'
    />
    <path d='m174.827 187.866-46.831 5.96v-78.048l46.831 5.874z' fill='#e05243' />
    <path d='m174.827 89.631-46.831 8.535-46.818-8.535 46.759-12.256z' fill='#5e1f18' />
    <path d='m174.827 219.801-46.831-8.591-46.818 8.591 46.761 13.053z' fill='#f2b0a9' />
    <path
      d='m81.178 89.631 46.818-11.586.379-.117v-77.615l-.379-.313-46.818 23.413z'
      fill='#8c3123'
    />
    <path d='m174.827 89.631-46.831-11.586v-78.045l46.831 23.413z' fill='#e05243' />
    <path
      d='m127.996 309.428-46.823-23.405v-66.217l46.823 11.582.689.783-.187 75.906z'
      fill='#8c3123'
    />
    <g fill='#e05243'>
      <path d='m127.996 309.428 46.827-23.405v-66.217l-46.827 11.582z' />
      <path d='m235.367 53.686 20.633 10.314v181.02l-20.633 10.31z' />
    </g>
  </svg>`}
/>

{/* MANUAL-CONTENT-START:intro */}
[Amazon S3](https://aws.amazon.com/s3/) is a highly scalable, secure, and durable cloud storage service provided by Amazon Web Services. It's designed to store and retrieve any amount of data from anywhere on the web, making it one of the most widely used cloud storage solutions for businesses of all sizes.

With Amazon S3, you can:

- **Store unlimited data**: Upload files of any size and type with virtually unlimited storage capacity
- **Access from anywhere**: Retrieve your files from anywhere in the world with low-latency access
- **Ensure data durability**: Benefit from 99.999999999% (11 9's) durability with automatic data replication
- **Control access**: Manage permissions and access controls with fine-grained security policies
- **Scale automatically**: Handle varying workloads without manual intervention or capacity planning
- **Integrate seamlessly**: Connect with other AWS services and third-party applications easily
- **Optimize costs**: Choose from multiple storage classes to optimize costs based on access patterns

In Sim, the S3 integration enables your agents to retrieve and access files stored in your Amazon S3 buckets using secure presigned URLs. This allows for powerful automation scenarios such as processing documents, analyzing stored data, retrieving configuration files, and accessing media content as part of your workflows. Your agents can securely fetch files from S3 without exposing your AWS credentials, making it easy to incorporate cloud-stored assets into your automation processes. This integration bridges the gap between your cloud storage and AI workflows, enabling seamless access to your stored data while maintaining security best practices through AWS's robust authentication mechanisms.
{/* MANUAL-CONTENT-END */}


## Operations

### `s3_get_object`

Retrieve an object from an AWS S3 bucket

#### Input

| Parameter | Type | Required | Description |
| --------- | ---- | -------- | ----------- |
| `accessKeyId` | string | Yes | Your AWS Access Key ID |
| `secretAccessKey` | string | Yes | Your AWS Secret Access Key |
| `s3Uri` | string | Yes | S3 Object URL |

#### Output

| Parameter | Type | Description |
| --------- | ---- | ----------- |
| `url` | string | Pre-signed URL for downloading the S3 object |
| `metadata` | object | File metadata including type, size, name, and last modified date |



## Best Practices

{/* MANUAL-CONTENT-START:bestPractices */}
When integrating the S3 block into your Sim.ai workflows, following established best practices ensures secure file access, optimal performance, and reliable data retrieval. The S3 block serves as a critical bridge between your automated workflows and Amazon S3 storage, enabling seamless file access through presigned URLs while maintaining security and performance standards.

**Connection Tag Usage and Data Flow**

Proper connection tag implementation is essential for effective S3 integrations. The `<s3.url>` connection tag provides the presigned URL that can be consumed by downstream blocks for file processing. Connect this output to Function blocks using `<function1.s3Url>` for custom processing or to Agent blocks using `<agent1.fileUrl>` for AI-powered document analysis. The `<s3.metadata>` connection tag contains valuable file information including size, type, and modification dates that can be used for conditional logic in workflow branches.

For input configurations, dynamically populate the `<s3.s3Uri>` field using outputs from previous blocks such as `<webhook.s3Path>` or `<function1.generatedUri>`. This pattern enables workflows that process multiple files based on external triggers or user inputs. Always validate S3 URI format before passing to the block to prevent runtime failures.

**Workflow Architecture Patterns**

Successful S3 workflows typically follow three primary architectural patterns. The **Webhook → S3 → Agent** pattern enables document processing workflows where external systems trigger file analysis. This pattern works exceptionally well for automated document review and content extraction workflows.

The **S3 → Function → Database** pattern supports file processing and metadata storage workflows. Use `<s3.metadata>` to extract file properties and `<s3.url>` to download content for processing in Function blocks. This architecture excels at building file indexing and content management systems.

For notification systems, implement **S3 → Function → Telegram** patterns where file availability triggers formatted notifications. Extract filename and size from `<s3.metadata>` to create informative messages about newly available files.

**Message/Data Formatting Excellence**

S3 URI formatting requires precise structure for successful operations. Always use the complete S3 URI format: `s3://bucket-name/path/to/file.extension`. Validate URI structure before workflow execution to prevent authentication errors. The S3 block outputs standardized metadata in JSON format containing `type`, `size`, `name`, and `lastModified` fields that can be directly consumed by downstream blocks.

When processing metadata, handle file size formatting appropriately for user-facing outputs. Convert byte values to human-readable formats (KB, MB, GB) in Function blocks before passing to notification systems. Parse the `lastModified` timestamp for time-based conditional logic in workflow branches.

**Security Considerations**

AWS credential management represents the most critical security aspect of S3 block implementation. Store `accessKeyId` and `secretAccessKey` in Sim.ai's secure environment variables rather than hardcoding them in workflows. Use IAM roles with minimal required permissions - typically `s3:GetObject` for the specific bucket and path pattern needed by your workflow.

Presigned URLs generated by the S3 block have limited validity periods for security. Plan downstream processing to consume the `<s3.url>` output promptly after generation. For workflows with extended processing times, implement retry mechanisms that regenerate presigned URLs if initial attempts fail due to expiration.

Never log or expose AWS credentials in workflow outputs. Implement error handling that masks credential information in debug outputs while still providing actionable error messages for troubleshooting.

**Performance Optimization**

Optimize S3 block performance through strategic placement in workflow architecture. Position S3 blocks early in workflows to minimize latency for file-dependent operations. Cache metadata from `<s3.metadata>` in Function blocks when multiple downstream operations need the same file information, reducing redundant S3 calls.

For workflows processing multiple files, implement parallel execution patterns where multiple S3 blocks operate concurrently rather than sequentially. This approach significantly reduces total workflow execution time for bulk file operations.

Consider file size implications when designing workflows. Large files may require extended download times that affect overall workflow performance. Implement timeout handling for S3 operations and consider alternative approaches for extremely large files that might exceed workflow execution limits.

**Debugging and Monitoring**

Comprehensive monitoring of S3 block operations requires tracking both successful operations and failure modes. Monitor the presence and validity of `<s3.url>` outputs to verify successful presigned URL generation. Empty or malformed URLs typically indicate credential issues or incorrect S3 URI formatting.

Implement logging for `<s3.metadata>` contents to verify file accessibility and properties. Missing metadata often indicates file permissions issues or incorrect S3 paths. Use metadata validation in Function blocks to detect and handle edge cases like zero-byte files or files with unexpected extensions.

For credential-related failures, implement error handling that provides specific guidance without exposing sensitive information. Common failure patterns include expired credentials, insufficient permissions, or incorrect bucket configurations. Create fallback mechanisms that can retry operations with alternative credential sets or escalate issues to administrators.

Track presigned URL usage patterns to optimize caching strategies and identify potential security issues like URLs being accessed from unexpected sources or after extended delays.
{/* MANUAL-CONTENT-END */}


## FAQ

{/* MANUAL-CONTENT-START:faq */}
<Accordions type="single">
  <Accordion title="How do I retrieve files from an S3 bucket in my Sim.ai workflow?">

    To retrieve files from Amazon S3, add an **S3 block** to your workflow and configure the required authentication and file location inputs:

    #### Required Inputs

    | Input | Description | Example |
    |-------|-------------|---------|
    | `accessKeyId` | Your AWS Access Key ID | `<start.awsAccessKey>` or environment variable |
    | `secretAccessKey` | Your AWS Secret Access Key | `<start.awsSecretKey>` or environment variable |
    | `s3Uri` | Full S3 object path | `<start.s3Uri>` or `s3://bucket-name/path/file.pdf` |
  </Accordion>
  <Accordion title="What is the correct format for the S3 URI input when connecting blocks?">

    The S3 URI must follow the standard S3 format: `s3://bucket-name/path/to/file`. You can provide this in several ways:

    #### Input Methods

    ```
    Static URI: s3://my-bucket/documents/report.pdf
    From webhook: <start.s3Uri>
    From function: <function1.generatedPath>
    Concatenated: s3://my-bucket/<start.fileName>
    ```

    The S3 block will parse this URI to extract the bucket name and object key for the AWS API call.
  </Accordion>
  <Accordion title="How do I reference the S3 block's outputs in downstream blocks?">

    Every S3 block exposes two main outputs that can be referenced using connection tags:

    #### Available Outputs

    | Output Tag | Type | Description |
    |------------|------|-------------|
    | `<s31.url>` | String | Presigned URL for direct file download/access |
    | `<s31.metadata>` | Object | File information including size, type, and timestamps |
    | `<s31.metadata.contentType>` | String | MIME type of the retrieved file |
    | `<s31.metadata.contentLength>` | Number | File size in bytes |
    | `<s31.metadata.lastModified>` | String | When the file was last modified |

    These outputs can be connected to Agent blocks for file analysis or Webhook responses for file sharing.
  </Accordion>
  <Accordion title="How do I connect an S3 block with an Agent block to analyze retrieved files?">

    To have an AI agent analyze S3 files, connect the S3 block's outputs to the Agent block's inputs:

    #### Example Configuration

    1. **S3 Block**: Retrieves the file and generates presigned URL
    2. **Agent Block**: Configure with:
       - `userPrompt`: `"Analyze this file: <s31.url>"`
       - `systemPrompt`: `"You are a file analysis assistant. Download and analyze the provided file."`

    #### Workflow Pattern
    ```
    Webhook → S3 → Agent → Response
    ```

    The agent can access the file directly using `<s31.url>` and provide analysis based on the content.
  </Accordion>
  <Accordion title="Why is my S3 block failing to retrieve files?">

    Troubleshoot common S3 access issues with this checklist:

    #### Common Issues and Solutions

    | Issue | Description | Solution |
    |-------|-------------|----------|
    | **Access Denied** | Invalid AWS credentials | Verify `<start.accessKeyId>` and `<start.secretAccessKey>` are correct |
    | **Bucket Not Found** | Incorrect bucket name in URI | Check S3 URI format: `s3://correct-bucket-name/file.txt` |
    | **Object Not Found** | File doesn't exist at specified path | Verify the complete object key in your S3 URI |
    | **Permission Error** | IAM user lacks S3 GetObject permission | Ensure AWS credentials have `s3:GetObject` permission for the bucket |
    | **Region Mismatch** | Bucket in different AWS region | AWS credentials must have access to the bucket's region |
  </Accordion>
  <Accordion title="Can I use the S3 block with file processing workflows?">

    Yes! The S3 block integrates well with other Sim.ai blocks for comprehensive file processing:

    #### Common Workflow Patterns

    ##### 1. **Webhook → S3 → Agent → Response**
    Process uploaded files with AI analysis and return results.

    ##### 2. **Scheduled Trigger → S3 → Function → Notification**
    Regularly check S3 files and process new uploads automatically.

    ##### 3. **S3 → Condition → Multiple Paths**
    Route different file types to specialized processing:
    ```
    if `<s31.metadata.contentType>` contains "image":
      → Image Processing Agent
    else:
      → Document Processing Agent
    ```

    ##### 4. **API → S3 → Agent → Database**
    Fetch file references from external APIs, retrieve from S3, analyze with AI, and store results.
  </Accordion>
  <Accordion title="How do I handle different file types when retrieving from S3?">

    Use the S3 block's metadata output to determine file processing logic:

    #### File Type Detection

    The `<s31.metadata.contentType>` output provides MIME type information:

    ```json
    {
      "contentType": "application/pdf",
      "contentLength": 2048576,
      "lastModified": "2024-01-15T10:30:00Z"
    }
    ```

    #### Conditional Processing Example

    Connect `<s31.metadata.contentType>` to a Condition block to route different file types:
    - PDFs → Document analysis agent
    - Images → Vision analysis agent  
    - CSV files → Data processing function

    This allows your workflow to automatically handle various file formats retrieved from S3.
  </Accordion>
</Accordions>
{/* MANUAL-CONTENT-END */}
