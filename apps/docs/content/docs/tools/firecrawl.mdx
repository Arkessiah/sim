---
title: Firecrawl
description: Scrape or search the web
---

import { BlockInfoCard } from "@/components/ui/block-info-card"
import { Accordion, Accordions } from "fumadocs-ui/components/accordion"


<BlockInfoCard 
  type="firecrawl"
  color="#181C1E"
  icon={true}
  iconSvg={`<svg className="block-icon" viewBox='0 0 642 600' xmlns='http://www.w3.org/2000/svg' >
      <path
        d='M301 63C299 91 303 122 298 149C295 158 289 165 283 169C274 172 266 170 261 167C253 176 248 183 244 191C230 226 226 263 226 301C216 310 203 317 192 310C179 295 175 277 174 259C161 273 153 288 146 304C141 321 138 336 137 352C140 372 145 388 152 402C161 421 174 435 187 449C181 462 165 453 157 450C158 454 161 458 165 461C195 490 231 500 268 509C240 494 211 471 195 442C179 413 172 378 180 344C191 353 200 362 211 364C223 365 232 361 236 353C247 274 299 214 323 143C322 136 327 140 329 142C354 165 367 191 375 218C387 254 381 294 379 329C393 345 413 334 424 329C429 342 432 352 429 362C427 378 417 388 413 400C422 407 433 403 440 400C432 423 419 442 404 460C383 483 358 501 335 512C379 502 420 491 449 459C443 458 427 464 428 452C443 437 464 423 472 403C482 383 485 362 484 339C482 307 472 280 458 254C459 267 452 276 445 284C434 289 426 279 424 272C415 247 424 220 418 198C415 179 405 165 397 150C370 114 336 86 303 64'
        fill='rgb(253,76,31)'
      />
      <path
        d='M324 141C303 214 249 273 244 354C235 359 229 364 223 366C205 367 193 357 182 347C180 350 179 353 180 357C178 374 178 390 182 403C185 421 193 434 200 448C212 465 227 480 243 491C258 500 269 513 285 512C284 508 257 485 252 468C241 450 235 433 233 414C241 415 254 420 263 412C260 387 265 363 273 343C281 323 293 306 310 295C317 289 324 285 330 282C328 307 328 331 329 355C330 368 332 379 338 389C358 394 376 384 388 370C383 386 377 401 371 415C376 414 381 411 385 408C383 421 380 431 376 441C366 467 356 491 334 510C358 499 381 483 400 461C418 442 430 423 440 403C432 404 421 410 413 404C414 386 428 377 427 360C429 349 428 340 424 332C413 336 404 341 392 339C386 338 381 334 379 330C380 292 385 248 371 214C366 195 358 180 349 165C341 155 333 145 323 140'
        fill='rgb(254,156,69)'
      />
      <path
        d='M330 284C309 293 289 311 279 332C267 356 261 383 265 411C256 420 242 418 235 412C237 438 245 459 258 479C269 493 281 507 295 513C288 495 265 472 265 446C272 447 281 454 288 444C296 425 303 407 309 388C317 406 321 427 336 443C346 449 358 446 363 438C355 464 348 489 334 511C344 501 352 491 357 480C370 457 379 435 385 412C380 411 376 416 371 418C376 401 382 386 387 371C379 382 369 388 358 391C348 394 337 392 334 383C324 353 328 316 330 285'
        fill='rgb(254,220,87)'
      />
      <path
        d='M311 389C303 407 297 426 289 445C282 454 273 450 268 445C267 472 285 492 302 512C299 514 297 514 294 514C297 514 299 514 301 514C314 515 325 512 334 513C341 495 355 467 362 443C357 446 351 448 344 447C337 446 334 441 330 438C320 422 316 406 311 391'
        fill='rgb(251,250,202)'
      />
      <path
        d='M187 163C188 181 167 187 164 203C158 215 158 228 159 241C172 233 183 221 188 209C193 194 192 178 188 166'
        fill='rgb(253,76,31)'
      />
    </svg>`}
/>

{/* MANUAL-CONTENT-START:intro */}
[Firecrawl](https://firecrawl.dev/) is a powerful web scraping and content extraction API that integrates seamlessly into Sim, enabling developers to extract clean, structured content from any website. This integration provides a simple way to transform web pages into usable data formats like Markdown and HTML while preserving the essential content.

With Firecrawl in Sim, you can:

- **Extract clean content**: Remove ads, navigation elements, and other distractions to get just the main content
- **Convert to structured formats**: Transform web pages into Markdown, HTML, or JSON
- **Capture metadata**: Extract SEO metadata, Open Graph tags, and other page information
- **Handle JavaScript-heavy sites**: Process content from modern web applications that rely on JavaScript
- **Filter content**: Focus on specific parts of a page using CSS selectors
- **Process at scale**: Handle high-volume scraping needs with a reliable API
- **Search the web**: Perform intelligent web searches and retrieve structured results
- **Crawl entire sites**: Crawl multiple pages from a website and aggregate their content

In Sim, the Firecrawl integration enables your agents to access and process web content programmatically as part of their workflows. Supported operations include:

- **Scrape**: Extract structured content (Markdown, HTML, metadata) from a single web page.
- **Search**: Search the web for information using Firecrawl's intelligent search capabilities.
- **Crawl**: Crawl multiple pages from a website, returning structured content and metadata for each page.

This allows your agents to gather information from websites, extract structured data, and use that information to make decisions or generate insights—all without having to navigate the complexities of raw HTML parsing or browser automation. Simply configure the Firecrawl block with your API key, select the operation (Scrape, Search, or Crawl), and provide the relevant parameters. Your agents can immediately begin working with web content in a clean, structured format.
{/* MANUAL-CONTENT-END */}


## Operations

### `firecrawl_scrape`

Extract structured content from web pages with comprehensive metadata support. Converts content to markdown or HTML while capturing SEO metadata, Open Graph tags, and page information.

#### Input

| Parameter | Type | Required | Description |
| --------- | ---- | -------- | ----------- |
| `url` | string | Yes | The URL to scrape content from |
| `scrapeOptions` | json | No | Options for content scraping |
| `apiKey` | string | Yes | Firecrawl API key |

#### Output

| Parameter | Type | Description |
| --------- | ---- | ----------- |
| `markdown` | string | Page content in markdown format |
| `html` | string | Raw HTML content of the page |
| `metadata` | object | Page metadata including SEO and Open Graph information |

### `firecrawl_search`

Search for information on the web using Firecrawl

#### Input

| Parameter | Type | Required | Description |
| --------- | ---- | -------- | ----------- |
| `query` | string | Yes | The search query to use |
| `apiKey` | string | Yes | Firecrawl API key |

#### Output

| Parameter | Type | Description |
| --------- | ---- | ----------- |
| `data` | array | Search results data |

### `firecrawl_crawl`

Crawl entire websites and extract structured content from all accessible pages

#### Input

| Parameter | Type | Required | Description |
| --------- | ---- | -------- | ----------- |
| `url` | string | Yes | The website URL to crawl |
| `limit` | number | No | Maximum number of pages to crawl \(default: 100\) |
| `onlyMainContent` | boolean | No | Extract only main content from pages |
| `apiKey` | string | Yes | Firecrawl API Key |

#### Output

| Parameter | Type | Description |
| --------- | ---- | ----------- |
| `pages` | array | Array of crawled pages with their content and metadata |



## Best Practices

{/* MANUAL-CONTENT-START:bestPractices */}
When implementing the Firecrawl block in your Sim.ai workflows, following established best practices ensures reliable web scraping, efficient data extraction, and optimal resource utilization. The Firecrawl block serves as a versatile web intelligence component, enabling automated content extraction, comprehensive website crawling, and intelligent web search capabilities within your automated workflows.

**Connection Tag Usage and Data Flow**

The Firecrawl block offers multiple output connection tags optimized for different use cases. Use `<firecrawl1.markdown>` when routing structured content to AI agents or content processing blocks, as markdown format provides clean, readable text that maintains formatting hierarchy. The `<firecrawl1.html>` output serves specialized use cases where raw HTML structure is required for custom parsing or template generation.

For metadata-driven workflows, leverage `<firecrawl1.metadata>` to access comprehensive page information including SEO titles, descriptions, and Open Graph properties. This connection tag proves invaluable for content analysis workflows where you need to extract specific metadata fields like `<firecrawl1.metadata.title>` or `<firecrawl1.metadata.description>` for downstream processing.

When implementing search-based workflows, connect `<firecrawl1.data>` to Function blocks for result filtering and ranking. For crawling operations, use `<firecrawl1.pages>` to access the complete array of discovered pages, while `<firecrawl1.total>` provides count information for workflow logic decisions.

**Workflow Architecture Patterns**

Successful Firecrawl implementations follow several proven architectural patterns. The most common pattern involves **Function → Firecrawl → Agent** flows, where Function blocks generate dynamic URLs or search queries, Firecrawl extracts content, and AI agents analyze or summarize the results. This pattern enables intelligent content monitoring and automated research workflows.

For comprehensive site analysis, implement **API → Firecrawl (crawl) → Function → Database** patterns. Start with a base URL, crawl the entire site, process the results through filtering functions, and store structured data for future analysis. This approach works exceptionally well for competitive intelligence and content auditing workflows.

Research automation follows **Agent → Firecrawl (search) → Function → Agent** patterns, where an initial AI agent formulates search queries, Firecrawl retrieves relevant content, functions filter and rank results, and a final agent synthesizes findings into actionable insights.

**Data Formatting Excellence and Input Optimization**

Structure your URL inputs to maximize extraction quality. Always use complete URLs with proper protocol specification (https://) and avoid URLs with authentication requirements or dynamic parameters that might interfere with content extraction. When using the scrapeOptions parameter, configure `onlyMainContent: true` for cleaner content extraction that focuses on primary page content while filtering out navigation, advertisements, and boilerplate elements.

For search operations, formulate queries using specific, targeted terms rather than broad phrases. Firecrawl's search functionality performs better with precise queries that include relevant keywords and context. Structure search queries to match natural language patterns when possible, as this improves result relevance and quality.

When implementing crawling workflows, set appropriate limit values based on your use case and API quota constraints. Start with conservative limits (10-50 pages) for testing, then scale based on actual requirements and `<firecrawl1.creditsUsed>` monitoring results.

**Debugging and Monitoring**

Implement comprehensive monitoring using Firecrawl's built-in output fields. Track `<firecrawl1.creditsUsed>` to monitor API consumption and prevent quota exhaustion. Set up alerting when credit usage exceeds expected thresholds, particularly for high-volume crawling operations.

Monitor `<firecrawl1.warning>` outputs to identify potential issues with URL accessibility, content extraction problems, or rate limiting. These warnings provide early indicators of workflow issues before they result in failures.

For scraping operations, validate that `<firecrawl1.markdown>` contains meaningful content by checking string length and content patterns. Empty or extremely short responses often indicate blocked access or content extraction failures. Implement conditional logic to retry failed extractions or route to alternative data sources.

Use the `<firecrawl1.total>` output in crawling workflows to verify expected page discovery. Significant deviations from expected page counts may indicate site structure changes or access restrictions that require workflow adjustments.

**Security and Access Management**

Store your Firecrawl API keys securely using Sim.ai's environment variable system rather than hardcoding them in workflows. Rotate API keys regularly and monitor usage patterns to detect unauthorized access or unusual consumption spikes.

Respect website robots.txt files and terms of service when implementing crawling workflows. While Firecrawl handles technical aspects of web scraping, ensure your use cases comply with target sites' access policies and data usage terms.

Implement rate limiting in high-frequency workflows to avoid overwhelming target websites or triggering anti-scraping measures. Use delays between requests and monitor for HTTP error responses that might indicate rate limiting or access blocks.

**Performance Optimization and Resource Management**

Optimize crawling operations by setting appropriate limits based on actual content requirements rather than maximizing page counts. Use `onlyMainContent: true` consistently to reduce processing overhead and improve extraction speed while maintaining content quality.

For large-scale crawling projects, implement pagination strategies that break extensive crawls into smaller batches. This approach improves workflow reliability and enables better error recovery when issues occur with specific page subsets.

Cache frequently accessed content by storing Firecrawl outputs in databases or file storage blocks. This reduces API consumption for repeated access to the same content and improves workflow response times.

Monitor `<firecrawl1.creditsUsed>` patterns to optimize workflow efficiency. Identify high-consumption operations and evaluate whether alternative approaches or reduced scope can achieve similar results with lower resource utilization.

Structure error handling to gracefully manage failed extractions or inaccessible URLs. Implement fallback mechanisms that can continue workflow execution even when specific pages or sites become unavailable, ensuring robust automation that adapts to changing web conditions.
{/* MANUAL-CONTENT-END */}


## FAQ

{/* MANUAL-CONTENT-START:faq */}
<Accordions type="single">
  <Accordion title="How do I scrape content from a website using the Firecrawl block in my workflow?">

    To scrape website content with Firecrawl, add a **Firecrawl block** to your Sim.ai workflow and configure the scraping operation:

    #### Required Inputs for Scraping

    | Input | Description | Example Connection |
    |-------|-------------|-------------------|
    | `url` | Target website URL to scrape | `<start.url>` or static URL like `https://example.com` |
    | `apiKey` | Your Firecrawl API key | `<start.apiKey>` or environment variable |
    | `scrapeOptions` | JSON scraping configuration | `<function1.options>` or static JSON object |

    The block will extract clean, structured content and return it in both markdown and HTML formats along with comprehensive metadata.
  </Accordion>
  <Accordion title="What outputs can I reference from a Firecrawl scraping operation?">

    Every Firecrawl block exposes multiple outputs that can be connected to downstream blocks:

    #### Available Scraping Outputs

    | Output Tag | Type | Description |
    |------------|------|-------------|
    | `<firecrawl1.markdown>` | String | Clean page content in markdown format |
    | `<firecrawl1.html>` | String | Raw HTML content of the scraped page |
    | `<firecrawl1.metadata>` | Object | SEO metadata, Open Graph tags, and page info |
    | `<firecrawl1.creditsUsed>` | Number | API credits consumed for the operation |
    | `<firecrawl1.warning>` | String | Any warnings encountered during scraping |

    These outputs can be dragged into Agent blocks, Function blocks, or other processing components.
  </Accordion>
  <Accordion title="How do I search the web for information using Firecrawl instead of scraping specific URLs?">

    To perform web searches rather than targeted scraping:

    1. **Set the operation** to `firecrawl_search` in your Firecrawl block
    2. **Configure search inputs**:

    #### Search Operation Inputs

    | Input | Description | Example |
    |-------|-------------|---------|
    | `query` | Your search terms | `<start.searchQuery>` or `<agent1.content>` |
    | `apiKey` | Firecrawl API key | `<start.apiKey>` |

    #### Search Results Output

    The search operation returns `<firecrawl1.data>` containing an array of search results with URLs, titles, and content snippets.
  </Accordion>
  <Accordion title="How do I connect a Firecrawl block to an Agent block for content analysis?">

    To analyze scraped content with AI:

    1. **Connect the content output** to the Agent's input:
       - Drag `<firecrawl1.markdown>` to `<agent1.userPrompt>`
       - Or combine with static text: `"Analyze this content: <firecrawl1.markdown>"`

    2. **Use metadata for context**:
       - Connect `<firecrawl1.metadata>` to provide page title, description, and SEO data

    #### Example Workflow Pattern
    ```
    Webhook → Firecrawl → Agent → Response
    ```
    Where `<start.url>` feeds Firecrawl, and `<firecrawl1.markdown>` feeds the Agent for analysis.
  </Accordion>
  <Accordion title="What is the difference between scraping, searching, and crawling operations in Firecrawl?">

    Firecrawl offers three distinct operations for different use cases:

    #### Operation Comparison

    | Operation | Purpose | Input | Output |
    |-----------|---------|-------|--------|
    | **Scrape** | Extract content from a single URL | `url`, `apiKey`, `scrapeOptions` | `markdown`, `html`, `metadata` |
    | **Search** | Find information across the web | `query`, `apiKey` | `data` (search results array) |
    | **Crawl** | Extract content from entire websites | `url`, `limit`, `onlyMainContent`, `apiKey` | `pages` (array of all crawled pages) |

    Choose scraping for single pages, searching for discovery, and crawling for comprehensive site analysis.
  </Accordion>
  <Accordion title="How do I crawl an entire website and process all pages with Firecrawl?">

    To crawl multiple pages from a website:

    1. **Set operation** to `firecrawl_crawl`
    2. **Configure crawling parameters**:

    #### Crawling Inputs

    | Input | Description | Example Value |
    |-------|-------------|---------------|
    | `url` | Base website URL to crawl | `<start.baseUrl>` |
    | `limit` | Maximum pages to crawl | `<start.pageLimit>` or static number like `50` |
    | `onlyMainContent` | Extract only main content | `true` or `<condition1.result>` |
    | `apiKey` | Firecrawl API key | `<start.apiKey>` |

    #### Crawling Outputs

    Access crawled data using:
    - `<firecrawl1.pages>` - Array of all crawled pages with content
    - `<firecrawl1.total>` - Total number of pages found
    - `<firecrawl1.creditsUsed>` - API credits consumed
  </Accordion>
  <Accordion title="Why is my Firecrawl operation failing or returning empty results?">

    Troubleshoot common Firecrawl issues with this checklist:

    #### Common Issues and Solutions

    | Issue | Cause | Solution |
    |-------|-------|----------|
    | **Invalid API Key** | Incorrect or expired API key | Verify `<start.apiKey>` is valid Firecrawl token |
    | **URL Format Error** | Malformed or inaccessible URL | Ensure `<start.url>` includes protocol (https://) |
    | **Rate Limiting** | Too many requests | Check `<firecrawl1.warning>` for rate limit messages |
    | **Empty Content** | Page blocked or JavaScript-heavy | Use `scrapeOptions` to handle dynamic content |
    | **Credit Exhaustion** | API credits depleted | Monitor `<firecrawl1.creditsUsed>` output |
    | **Network Issues** | Connection problems | Check workflow logs for HTTP errors |
  </Accordion>
  <Accordion title="Can I combine Firecrawl with other blocks for automated web monitoring?">

    Yes! Firecrawl integrates seamlessly with other Sim.ai blocks for comprehensive automation:

    #### Common Integration Patterns

    ##### 1. **Scheduled Monitoring**
    ```
    Schedule Trigger → Firecrawl → Agent → Condition → Telegram
    ```
    Monitor websites for changes and send alerts when content updates.

    ##### 2. **Research Pipeline**
    ```
    Webhook → Firecrawl (search) → Function → Firecrawl (scrape) → Agent
    ```
    Search for relevant URLs, then scrape and analyze the best results.

    ##### 3. **Content Processing**
    ```
    Firecrawl → Function → Database → Email
    ```
    Scrape content, transform data, store results, and notify stakeholders.

    ##### 4. **Competitive Analysis**
    ```
    API → Firecrawl (crawl) → Agent → Spreadsheet
    ```
    Crawl competitor sites, analyze with AI, and compile insights into reports.
  </Accordion>
</Accordions>
{/* MANUAL-CONTENT-END */}
